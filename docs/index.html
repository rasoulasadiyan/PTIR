<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PTIR: Persian Text-Image Retrieval Framework</title>
    <link href="https://fonts.googleapis.com/css2?family=Lora:wght@700&family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <link rel="stylesheet" href="styles.css">
    <style>
        body {
            font-family: 'Open Sans', sans-serif;  /* Changed to Open Sans */
            background-color: #fcfbf8;
            color: #252525;
            line-height: 1.4;
            margin: 0;
            padding: 0;
        }
    
        h1 {
            font-family: 'Merriweather', serif;  /* Changed to Merriweather */
            color: #ffffff;
            font-size: 2.5rem;  /* Reduced size */
            margin-bottom: 0.5rem;
        }
    
        h2, h3 {
            font-family: 'Merriweather', serif;  /* Changed to Merriweather */
            color: rgb(22, 21, 2);
            font-size: 2rem;  /* Reduced size */
            margin-bottom: 0.5rem;
        }
    
        p {
            font-size: 1rem;  /* Smaller font size for body text */
            text-align: justify;
        }

        pp {
            font-size: 1rem;  /* Smaller font size for body text */
        }
    
        .img-zoom {
            cursor: pointer;
            transition: transform 0.2s ease;  /* Slightly faster transition */
        }
    
        .img-zoom:hover {
            transform: scale(1.05);  /* Slightly smaller zoom effect */
        }
    
        .modal-dialog {
            max-width: 80%;
            margin: 1rem auto;
        }

        .modal-content {
            background-color: #f8f9fa;
        }

        .modal-body {
            display: flex;
            justify-content: center;
            align-items: center;
        }

        header {
            background: linear-gradient(135deg, #2980b9, #8e44ad);
            padding: 50px 0;
            color: white;
            text-align: center;
            box-shadow: 0 10px 15px rgba(0, 0, 0, 0.1);
        }

        header h2 {
            font-family: 'Lora', serif;
            font-weight: 700;
            font-size: 2.5rem;
        }

        header h1 {
            font-family: 'Lora', serif;
            font-weight: 700;
            font-size: 2rem;
        }

        .social-links .btn {
            margin-right: 10px;
        }

        .content h2 {
            font-family: 'Lora', serif;
            font-weight: 700;
        }

        .content p {
            font-family: 'Roboto', sans-serif;
        }

        section {
            margin-bottom: 40px;
        }

        .text-center {
            text-align: center;
        }

        footer {
            background-color: #343a40;
            color: #fff;
            padding: 20px;
        }

        .img-caption {
            font-size: 0.9rem;
            color: #666;
            text-align: center;
            margin-top: 10px;
        }


        .button-container {
            display: flex;
            gap: 15px;
            margin-top: 20px;
            justify-content: center;
        }
        .button {
            padding: 12px 24px;
            font-size: 16px;
            cursor: pointer;
            border: none;
            border-radius: 8px;
            display: flex;
            align-items: center;
            transition: background-color 0.3s ease;
        }
        .github-button {
            background-color: #525151;
            color: rgb(255, 255, 255);
        }
        .github-button:hover {
            background-color: #000000;
        }
        .huggingface-button {
            background-color: #ffee07;
            color: rgb(26, 25, 25);
        }
        .huggingface-button:hover {
            background-color: #ffd000;
        }
        .icon {
            width: 24px;
            height: 24px;
            margin-right: 10px;
            background-size: cover;
        }
        .github-icon {
            background-image: url('https://simpleicons.org/icons/github.svg');
            filter: invert(1);
        }
        .huggingface-icon {
            background-image: url('https://huggingface.co/favicon.ico');
        }
        
    </style>
</head>
<body>
    <header class="header">
        <div class="container text-center">
            <h1 class="mt-4 mb-3 text-white display-4">Persian Text-Image Retrieval</h1>
            <pp class="text-white-75 mb-3 lead">A Framework Based on Image Captioning and Scalable Vector Search</pp>
            <div class="button-container">
                <a href="https://github.com/rasoulasadiyan/PTIR" target="_blank">
                    <button class="button github-button">
                        <span class="icon github-icon"></span>GitHub
                    </button>
                </a>
                <a href="https://huggingface.co/datasets/rasoulub/coco-pic" target="_blank">
                    <button class="button huggingface-button">
                        <span class="icon huggingface-icon"></span>PIC Dataset
                    </button>
                </a>
            </div>
        </div>
    </header>

    <main class="content">
        <div class="container">
            <section class="mb-3">
                <h2 class="text-center">Intro</h2>
                <p class="lead">In recent years, Vision-Language Models (VLMs) have achieved remarkable success in connecting the domains of vision and language. Models such as CLIP and ALIGN, trained on vast datasets of image-text pairs, have enabled highly efficient cross-modal retrieval systems. However, these breakthroughs have primarily been limited to English and a few other widely spoken languages. Persian, is among the underserved languages in this regard, with limited resources and tools available for advanced tasks such as image-text retrieval.</p>
            </section>

            <section class="mb-3">
                <div class="text-center video-title-container">
                    <h2>PTIR Demo</h2> <!-- Title above the video -->
                </div>
                <div class="text-center">
                    <video width="1080" height="720" controls>
                        <source src="assets/PTIR_D4.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
            </section>

            <section class="mb-3">
                <h2 class="text-center">General Works in the Paper</h2>
                <p class="lead">While existing multilingual VLMs attempt to bridge the gap for non-English languages, their performance in Persian remains suboptimal due to the lack of high-quality datasets and models tailored for the language. Specifically, there are no comprehensive Persian text-image retrieval systems that can effectively handle detailed queries or retrieve relevant images in diverse real-world scenarios.</p>
                <p class="lead">Furthermore, widely adopted systems like CLIP are challenging to fine-tune or adapt to such specialized domains due to their reliance on large-scale datasets and resources unavailable in Persian.</p>
                <p class="lead">This paper proposes a pioneering approach to Persian Text-Image Retrieval (PTIR), marking a significant advancement in the field. Our contributions include:</p>
            </section>

            <section class="mb-5">
                <h2 class="text-center">A. Dataset </h2>
                <p class="lead">Our work introduces a groundbreaking dataset of 1.2 million Persian image-caption pairs, setting a new standard for Persian text-image retrieval with detailed, high-quality captions. Data collection involved aggregating diverse sources, generating captions using advanced Vision-Language Models, and refining them for cultural and linguistic accuracy. Unlike existing datasets, our captions are rich in detail, describing key visual elements like object counts, shapes, colors, environmental context, and unique attributes such as age groups and animal breeds, ensuring comprehensive and precise image descriptions.</p>
                <div class="row">
                    <div class="col-md-4 mb-4">
                        <img src="assets/train_val_dis_len.png" alt="Dataset Analysis" class="img-fluid img-zoom" data-toggle="modal" data-target="#imgModal" data-src="assets/train_val_dis_len.png">
                        <div class="img-caption">Comparison Train-Val caption lengths</div>
                    </div>
                    <div class="col-md-4 mb-4">
                        <img src="assets/Train Word Cloud.png" alt="Caption Length Distribution" class="img-fluid img-zoom" data-toggle="modal" data-target="#imgModal" data-src="assets/Train Word Cloud.png">
                        <div class="img-caption">Train Word Cloud</div>
                    </div>
                    <div class="col-md-4 mb-4">
                        <img src="assets/Vlidation Word Cloud.png" alt="Sample Captions" class="img-fluid img-zoom" data-toggle="modal" data-target="#imgModal" data-src="assets/Vlidation Word Cloud.png">
                        <div class="img-caption">Vlidation Word Cloud</div>
                    </div>
                </div>
            </section>

            <section class="mb-5">
                <h2 class="text-center">D. Model Development</h2>
                <p class="lead">PTIR’s captioning model uses DINOv2-base as a vision encoder and GPT2-fa as a Persian-specific text decoder. The two-phase training strategy first fine-tunes the text decoder, then the entire model, leading to 30-40% improvement in captioning performance over existing Persian models.</p>
                <div class="row">
                    <div class="col-md-4 mb-4">
                        <img src="assets/T10.png" alt="Sample Captions" class="img-fluid img-zoom" data-toggle="modal" data-target="#imgModal" data-src="assets/T10.png">
                        <div class="img-caption">Vision Encoders comparison</div>
                    </div>
                    <div class="col-md-4 mb-4">
                        <img src="assets/M.png" alt="Evaluation Results" class="img-fluid img-zoom" data-toggle="modal" data-target="#imgModal" data-src="assets/M.png">
                        <div class="img-caption">Model architecture</div>
                    </div>
                    <div class="col-md-4 mb-4">
                        <img src="assets/T1.png" alt="Hit@k Metric" class="img-fluid img-zoom" data-toggle="modal" data-target="#imgModal" data-src="assets/T1.png">
                        <div class="img-caption">PIC models comparison</div>
                    </div>
                </div>
            </section>

            <section class="mb-2">
                <h2 class="text-center">C. Retrieval Framework</h2>
                <p class="lead">Our retrieval pipeline integrates an image captioning model with a scalable vector database to enable efficient and accurate image-text retrieval. The process involves three key steps:</p>
                <ul>
                  <li><strong>Embedding Generation:</strong> Captions generated by the captioning model are transformed into dense embeddings using a sentence embedding model to capture semantic meaning.</li>
                  <li><strong>Vector Database:</strong> After evaluating several options, Milvus was selected for its scalability, speed, and integration ease, supporting large-scale similarity searches with dense embeddings.</li>
                  <li><strong>Query Processing:</strong> Text queries are embedded using the same model as captions, enabling consistent top-k retrieval of similar images via Milvus.</li>
                </ul>
                <p>This modular framework is highly adaptable for domain-specific applications, such as medical imaging or cultural heritage, where specialized datasets can improve performance beyond general-purpose models like CLIP.</p>

                <div class="text-center">
                    <img src="assets/PTIR.png" alt="Retrieval Process" class="img-fluid img-zoom" data-toggle="modal" data-target="#imgModal" data-src="assets/PTIR.png">
                    <div class="img-caption">Retrieval Process</div>
                </div>
            </section>

            <section class="mb-5">
                <h2 class="text-center">D. Retrieval Evaluation</h2>
                <p class="lead">We evaluated PTIR using Hit@K, where it outperforms Persian baselines and CLIP-based models. PTIR achieves 22% Hit@1 and 80% Hit@200, demonstrating strong retrieval performance.</p>
                <div class="row">
                    <div class="col-md-5 mb-4">
                        <img src="assets/T1.png" alt="Evaluation Results" class="img-fluid img-zoom" data-toggle="modal" data-target="#imgModal" data-src="assets/T1.png">
                        <div class="img-caption">Evaluation Results</div>
                    </div>
                    <div class="col-md-5 mb-4">
                        <img src="assets/T2.png" alt="Hit@k Metric" class="img-fluid img-zoom" data-toggle="modal" data-target="#imgModal" data-src="assets/T2.png">
                        <div class="img-caption">Hit@k Metric</div>
                    </div>
                </div>
            </section>

            <section class="mb-2">
                <h2 class="text-center">E. Computational Efficiency and Scalability</h2>
                <p class="lead">PTIR optimizes efficiency with 3ms retrieval latency and fast embedding storage. It is more scalable and resource-efficient compared to English-centric models like CLIP and SigLIP, making it practical for real-world Persian applications.</p>
                <div class="text-center">
                    <img src="assets/T6.png" alt="Model Architecture" class="img-fluid img-zoom" data-toggle="modal" data-target="#imgModal" data-src="assets/T6.png">
                    <div class="img-caption">Ours model comparison</div>
                </div>
            </section>

            <section class="mb-5">
                <h2 class="text-center">F. Future Work</h2>
                <p class="lead">Our work contributes a novel Persian Text-Image Retrieval framework that advances the state of the art for this underrepresented language. We demonstrate the system’s effectiveness in real-world scenarios and envision future improvements, including broader dataset expansions and optimizations for real-time applications.</p>
            </section>
        </div>
    </main>

    <div class="modal fade" id="imgModal" tabindex="-1" role="dialog" aria-labelledby="exampleModalCenterTitle" aria-hidden="true">
        <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
                <div class="modal-header">
                    <h5 class="modal-title" id="exampleModalLongTitle">Image Zoom</h5>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true">&times;</span>
                    </button>
                </div>
                <div class="modal-body">
                    <img src="" id="modalImage" alt="Modal Image" class="img-fluid">
                </div>
            </div>
        </div>
    </div>

    <footer>
        <div class="container text-center">
            <pp class="mb-0">© 2024 PTIR  |  Rasoul Asadian</pp>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.3/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
    <script>
        $(document).ready(function() {
            $('.img-zoom').click(function() {
                var src = $(this).data('src');
                $('#modalImage').attr('src', src);
            });
        });
    </script>
</body>
</html>
